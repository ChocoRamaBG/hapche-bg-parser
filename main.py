import time
import os
import pandas as pd
import csv
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException

# --- ‚öôÔ∏è –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø & BRAINROT ---
START_TIME = time.time()
TIME_LIMIT_SECONDS = 5.5 * 60 * 60  # 5 —á–∞—Å–∞ –∏ 30 –º–∏–Ω—É—Ç–∏ grind

output_dir = "scraped_data"
state_file = "last_page.txt" 
current_batch_filename = os.path.join(output_dir, f"hapche_data.csv")

if not os.path.exists(output_dir):
    try:
        os.makedirs(output_dir)
        print("üìÅ –ü–∞–ø–∫–∞—Ç–∞ –µ –≥–æ—Ç–æ–≤–∞. Skibidi dop yes.")
    except Exception as e:
        print(f"‚ö†Ô∏è –ì–†–ï–î–ê! –ù–µ –º–æ–≥–∞ –¥–∞ —Å—ä–∑–¥–∞–º –ø–∞–ø–∫–∞—Ç–∞: {e}")

# --- üìú –ß–ï–¢–ï–ù–ï –ù–ê STATE ---
start_page = 1
if os.path.exists(state_file):
    try:
        with open(state_file, "r") as f:
            content = f.read().strip()
            if content.isdigit():
                start_page = int(content)
                print(f"üîÑ Loading save game... Level {start_page}.")
    except Exception:
        print("‚ö†Ô∏è Corrupted save file. Starting fresh.")

# --- üìù –î–ï–§–ò–ù–ò–†–ê–ù–ï –ù–ê –ö–û–õ–û–ù–ò–¢–ï ---
fieldnames = [
    "–ò–º–µ", "URL", "–ì—Ä–∞–¥ (–¢–∞–±–ª–∏—Ü–∞)", "–°–ø–µ—Ü–∏–∞–ª–Ω–æ—Å—Ç (–ü—Ä–æ—Ñ–∏–ª)", 
    "–ü–æ—Å–µ—â–µ–Ω–∏—è (–ü—Ä–æ—Ñ–∏–ª)", "–†–µ–π—Ç–∏–Ω–≥ (–ü—Ä–æ—Ñ–∏–ª)", "–ì–ª–∞—Å–æ–≤–µ (–ü—Ä–æ—Ñ–∏–ª)", 
    "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏ (–ü—Ä–æ—Ñ–∏–ª)", "–ê–¥—Ä–µ—Å (–ü—Ä–æ—Ñ–∏–ª)", "–¢–µ–ª–µ—Ñ–æ–Ω–∏", 
    "–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ", "Email", "Website", "Timestamp"
]

if not os.path.exists(current_batch_filename):
    try:
        with open(current_batch_filename, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
    except Exception as e:
        print(f"‚ùå Error creating CSV: {e}")

# --- ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –ù–ê –ë–†–ê–£–ó–™–†–ê (GITHUB ACTIONS MODE) ---
options = Options()

# üõë –í–ê–ñ–ù–û –ó–ê GITHUB ACTIONS:
options.add_argument('--headless=new')  # <-- –¢–û–í–ê –ï –ó–ê–î–™–õ–ñ–ò–¢–ï–õ–ù–û –¢–ê–ú!
options.add_argument('--no-sandbox')    # <-- –¢–û–í–ê –°–™–©–û!
options.add_argument('--disable-dev-shm-usage') # <-- –¢–û–í–ê –°–ü–ê–°–Ø–í–ê –ü–ê–ú–ï–¢–¢–ê!
options.add_argument('--disable-gpu')   # <-- –ó–∞ –≤—Å–µ–∫–∏ —Å–ª—É—á–∞–π
options.add_argument('--window-size=1920,1080')
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--log-level=3')
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

print("‚è≥ Summoning Chrome Demon (Headless Mode)...")
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    print("‚úÖ –î—Ä–∞–π–≤—ä—Ä—ä—Ç –∑–∞—Ö–∞–ø–∞! Vamos!")
except Exception as e:
    print(f"üí• FATAL ERROR –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Chrome: {e}")
    # –ê–∫–æ –≥—Ä—ä–º–Ω–µ —Ç—É–∫, –Ω—è–º–∞ —Å–º–∏—Å—ä–ª –¥–∞ –ø—Ä–æ–¥—ä–ª–∂–∞–≤–∞–º–µ, –∑–∞—Ç–æ–≤–∞ exit
    exit(1)

# --- üç™ COOKIE MONSTER SLAYER ---
def nuke_cookie_popups(driver):
    """
    –¢—ä—Ä—Å–∏ –∏ —É–Ω–∏—â–æ–∂–∞–≤–∞ –±–∏—Å–∫–≤–∏—Ç—á–æ–≤—Ü–∏ –∏ GDPR –≥–ª—É–ø–æ—Å—Ç–∏.
    """
    # 1. Google Funding Choices
    try:
        accept_btn = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.fc-cta-consent"))
        )
        # –ò–∑–ø–æ–ª–∑–≤–∞–º–µ JavaScript click, –∑–∞—â–æ—Ç–æ –ø–æ–Ω—è–∫–æ–≥–∞ –µ–ª–µ–º–µ–Ω—Ç—ä—Ç –µ –∑–∞–∫—Ä–∏—Ç
        driver.execute_script("arguments[0].click();", accept_btn)
        print("üç™ Google Cookie Popup: DELETED via JS.")
    except TimeoutException:
        pass 
    except Exception as e:
        pass 

    # 2. TermsFeed Popup
    try:
        accept_btn = WebDriverWait(driver, 2).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button.cc-nb-okagree"))
        )
        driver.execute_script("arguments[0].click();", accept_btn)
        print("üç™ TermsFeed Popup: OBLITERATED via JS.")
    except TimeoutException:
        pass
    except Exception as e:
        pass

# --- üíæ –ó–ê–ü–ò–°–í–ê–ß–ö–ê–¢–ê ---
def save_single_record(record):
    if not record: return
    try:
        with open(current_batch_filename, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writerow(record)
        print(f"üíæ {record.get('–ò–º–µ', 'Unknown')} -> Saved. W.")
    except Exception as e:
        print(f"‚ùå Save error: {e}")

# --- üïµÔ∏è‚Äç‚ôÇÔ∏è AGENT 007 ---
def scrape_details_from_profile(url, basic_info):
    print(f"    üëâ Visiting: {url}")
    try:
        driver.get(url)
        
        # üí£ –£–±–∏–≤–∞–º–µ –±–∏—Å–∫–≤–∏—Ç–∫–∏—Ç–µ
        nuke_cookie_popups(driver)
        
        # –ß–∞–∫–∞–º–µ body-—Ç–æ
        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        except:
            print("‚ö†Ô∏è Page took too long or is broken.")

        # --- HERO SECTION ---
        try:
            basic_info["–ò–º–µ"] = driver.find_element(By.XPATH, "//h1[@itemprop='name']").text.strip()
        except: pass

        try:
            basic_info["–°–ø–µ—Ü–∏–∞–ª–Ω–æ—Å—Ç (–ü—Ä–æ—Ñ–∏–ª)"] = driver.find_element(By.CSS_SELECTOR, ".subtitle--category").text.strip()
        except: pass

        # --- STATISTICS ---
        stats_map = {
            "–ü–æ—Å–µ—â–µ–Ω–∏—è (–ü—Ä–æ—Ñ–∏–ª)": "visits-statistics-metadata-value",
            "–†–µ–π—Ç–∏–Ω–≥ (–ü—Ä–æ—Ñ–∏–ª)": "rating-statistics-metadata-value",
            "–ì–ª–∞—Å–æ–≤–µ (–ü—Ä–æ—Ñ–∏–ª)": "votes-statistics-metadata-value",
            "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏ (–ü—Ä–æ—Ñ–∏–ª)": "comments-statistics-metadata-value"
        }
        for key, div_id in stats_map.items():
            try:
                basic_info[key] = driver.find_element(By.ID, div_id).text.strip()
            except: basic_info[key] = "-"

        # --- KONTAKTI ---
        phones = []
        try:
            phone_container = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–¢–µ–ª–µ—Ñ–æ–Ω')]/following-sibling::div[contains(@class, 'value')]")
            phone_divs = phone_container.find_elements(By.TAG_NAME, "div")
            if phone_divs:
                phones = [p.text.strip() for p in phone_divs if p.text.strip()]
            else:
                phones = [phone_container.text.strip()]
        except: pass
        basic_info["–¢–µ–ª–µ—Ñ–æ–Ω–∏"] = ", ".join(phones) if phones else "-"

        # Adres
        address_profile = "-"
        try:
            address_profile = driver.find_element(By.ID, "address-value").text.strip().replace('\n', ', ')
        except:
            try:
                address_profile = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ê–¥—Ä–µ—Å')]/following-sibling::div[contains(@class, 'value')]").text.strip()
            except: pass
        basic_info["–ê–¥—Ä–µ—Å (–ü—Ä–æ—Ñ–∏–ª)"] = address_profile

        # Rabotno vreme & Email
        try:
            basic_info["–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ"] = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ')]/following-sibling::div[contains(@class, 'value')]").text.strip()
        except: basic_info["–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ"] = "-"

        try:
            basic_info["Email"] = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –ø–æ—â–∞')]/following-sibling::div[contains(@class, 'value')]").text.strip()
        except: basic_info["Email"] = "-"
        
        try:
            basic_info["Website"] = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü–∞')]/following-sibling::div[contains(@class, 'value')]//a").get_attribute("href")
        except: basic_info["Website"] = "-"

        basic_info["Timestamp"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return basic_info

    except Exception as e:
        print(f"üíÄ Profile error (–∞–Ω–¥–∏–±—É–ª –º–æ—Ä–∫–æ–≤): {e}")
        return basic_info

# --- üìú MAIN LOOP ---
page = start_page
print(f"üöÄ Starting grind from page {page}.")

try:
    while True:
        if (time.time() - START_TIME) > TIME_LIMIT_SECONDS:
            print("üõë Time limit reached.")
            break

        target_url = f"https://www.rating.hapche.bg/search/lekari-spetsialisti/-/-&page={page}"
        print(f"\nüìÑ --- PAGE {page} ---")
        
        try:
            driver.get(target_url)
            
            # –ú–∞—Ö–∞–º–µ –±–∏—Å–∫–≤–∏—Ç–∫–∏—Ç–µ
            nuke_cookie_popups(driver)

            try:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.mr-table")))
            except:
                print("‚õî No table found. End of the line.")
                break

            rows = driver.find_elements(By.CSS_SELECTOR, "table.mr-table tbody tr")
            if not rows:
                print("‚õî No doctors found.")
                break

            print(f"üîé Found {len(rows)} potential victims (doctors).")
            
            doctors_on_page = []
            for row in rows:
                try:
                    name_el = row.find_element(By.CSS_SELECTOR, "td.name a")
                    url = name_el.get_attribute("href")
                    name = name_el.text.strip()
                    
                    city = "-"
                    try:
                        details = row.find_element(By.CSS_SELECTOR, "td.name span").text
                        if "–≥—Ä." in details:
                            city = "–≥—Ä. " + details.split("–≥—Ä.")[1].split(",")[0].strip()
                    except: pass

                    if "search" not in url:
                        doctors_on_page.append({
                            "–ò–º–µ": name, 
                            "URL": url,
                            "–ì—Ä–∞–¥ (–¢–∞–±–ª–∏—Ü–∞)": city
                        })
                except: continue

            # –í–ª–∏–∑–∞–º–µ –≤—ä–≤ –≤—Å–µ–∫–∏
            for doc in doctors_on_page:
                full_data = scrape_details_from_profile(doc['URL'], doc)
                save_single_record(full_data)

            page += 1
            with open(state_file, "w") as f:
                f.write(str(page))

        except Exception as e:
            print(f"ü§¨ Page error: {e}")
            page += 1 

finally:
    try: driver.quit()
    except: pass
    print(f"\nüèÅ Finished. Last page: {page}.")
