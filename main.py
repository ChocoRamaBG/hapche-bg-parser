import time
import os
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- üìÅ –ü–™–¢ –ö–™–ú –ü–ê–ü–ö–ò–¢–ï (PATHCHOVTSI) ---
# –í –æ–±–ª–∞–∫–∞ (GitHub Actions) –ø–∏—à–µ–º –≤ —Ç–µ–∫—É—â–∞—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
output_dir = "scraped_data"

if not os.path.exists(output_dir):
    try:
        os.makedirs(output_dir)
        print("üìÅ –ü–∞–ø–∫–∞—Ç–∞ –Ω–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—à–µ, –∞–º–∞ –∞–∑ —Å—ä–º sigma male –∏ —Ç–∏ —è —Å—ä–∑–¥–∞–¥–æ—Ö.")
    except Exception as e:
        print(f"‚ö†Ô∏è –ì–†–ï–î–ê! –ù–µ –º–æ–≥–∞ –¥–∞ —Å—ä–∑–¥–∞–º –ø–∞–ø–∫–∞—Ç–∞. Linux —Å–µ –ø—Ä–∞–≤–∏ –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å–µ–Ω: {e}")

output_filename = os.path.join(output_dir, "hapche_PRO_GRIND_SAVE.xlsx")
print(f"üéØ –§–∞–π–ª—ä—Ç —â–µ —Å–µ –∫–∞–∑–≤–∞: {output_filename}")

# --- ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –ù–ê –ë–†–ê–£–ó–™–†–ê –ó–ê –û–ë–õ–ê–ö–ê ---
options = Options()
# –¢–û–í–ê –ï –í–ê–ñ–ù–û, –õ–¨–û–õ–¨–û! –ë–µ–∑ —Ç–æ–≤–∞ –≤ GitHub Actions –Ω–∏—â–æ –Ω—è–º–∞ –¥–∞ —Å—Ç–∞–Ω–µ.
options.add_argument('--headless=new')  # –ë–µ–∑ –≥—Ä–∞—Ñ–∏—á–µ–Ω –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –∫–∞—Ç–æ –¥—É—à–∞—Ç–∞ –º–∏
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--window-size=1920,1080')
options.add_argument('--disable-blink-features=AutomationControlled')
options.add_argument('--log-level=3')
# –ú–∞–ª–∫–æ fake user-agent, –¥–∞ –Ω–µ –Ω–∏ —Ö–≤–∞–Ω–∞—Ç –≤–µ–¥–Ω–∞–≥–∞, —á–µ —Å–º–µ –±–æ—Ç—á–æ–≤—Ü–∏
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

# --- üöó –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –î–†–ê–ô–í–™–†–ß–û–í–¶–ò ---
print("‚è≥ –ü–∞–ª—è –≥—É–º–∏—Ç–µ –Ω–∞ Chrome –≤ –æ–±–ª–∞–∫–∞... Skibidi dop dop!")
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    print("‚úÖ –î—Ä–∞–π–≤—ä—Ä—ä—Ç –∑–∞—Ä–µ–¥–∏. Rizz level: 1000.")
except Exception as e:
    print(f"üí• –ú–∞–º–∫–∞ –º—É —á–æ–≤–µ—á–µ, –¥—Ä–∞–π–≤—ä—Ä—ä—Ç –≥—Ä—ä–º–Ω–∞: {e}")
    # –ü—Ä–æ–±–≤–∞–º–µ –ø–∞–∫ –±–µ–∑ —Å—ä—Ä–≤–∏—Å –º–µ–Ω–∏–¥–∂—ä—Ä–∞, –∞–∫–æ –≥—Ä—ä–º–Ω–µ (–º–∞–ª–∏–Ω–∏ –∏ –∫—ä–ø–∏–Ω–∏, –≤—Å–µ —Ç–∞—è)
    driver = webdriver.Chrome(options=options)

# --- üíæ –ó–ê–ü–ò–°–í–ê–ß–ö–ê–¢–ê ---
def save_single_record(record):
    if not record: return
    try:
        new_df = pd.DataFrame([record])
        if os.path.exists(output_filename):
            try:
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
            except:
                time.sleep(1)
                final_df = new_df 
        else:
            final_df = new_df

        final_df.to_excel(output_filename, index=False)
        print(f"üíæ –î–æ–∫—Ç–æ—Ä—ä—Ç '{record.get('–ò–º–µ')}' –µ –∑–∞–ø–∏—Å–∞–Ω. Stonks üìà.")
    except Exception as e:
        print(f"‚ùå ERROR –ø—Ä–∏ –∑–∞–ø–∏—Å: {e}. –î–∞–Ω–Ω–∏—Ç–µ –∏–∑—á–µ–∑–Ω–∞—Ö–∞ –≤ shadow realm-a.")

# --- üïµÔ∏è‚Äç‚ôÇÔ∏è AGENT 007 ---
def scrape_details_from_profile(url, basic_info):
    print(f"   üëâ Visiting: {url}")
    try:
        driver.get(url)
        # Brainrot wait time
        time.sleep(1.5) 

        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

        # --- HERO SECTION ---
        try:
            full_name = driver.find_element(By.XPATH, "//h1[@itemprop='name']").text.strip()
            basic_info["–ò–º–µ"] = full_name
        except: pass

        try:
            specialties_full = driver.find_element(By.CSS_SELECTOR, ".subtitle--category").text.strip()
            basic_info["–°–ø–µ—Ü–∏–∞–ª–Ω–æ—Å—Ç (–ü—Ä–æ—Ñ–∏–ª)"] = specialties_full
        except: pass

        # --- STATISTICS ---
        stats_map = {
            "–ü–æ—Å–µ—â–µ–Ω–∏—è (–ü—Ä–æ—Ñ–∏–ª)": "visits-statistics-metadata-value",
            "–†–µ–π—Ç–∏–Ω–≥ (–ü—Ä–æ—Ñ–∏–ª)": "rating-statistics-metadata-value",
            "–ì–ª–∞—Å–æ–≤–µ (–ü—Ä–æ—Ñ–∏–ª)": "votes-statistics-metadata-value",
            "–ö–æ–º–µ–Ω—Ç–∞—Ä–∏ (–ü—Ä–æ—Ñ–∏–ª)": "comments-statistics-metadata-value"
        }
        
        for key, div_id in stats_map.items():
            try:
                val = driver.find_element(By.ID, div_id).text.strip()
                basic_info[key] = val
            except: 
                basic_info[key] = "-"

        # --- –ö–û–ù–¢–ê–ö–¢–ò ---
        phones = []
        try:
            phone_container = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–¢–µ–ª–µ—Ñ–æ–Ω')]/following-sibling::div[contains(@class, 'value')]")
            phone_divs = phone_container.find_elements(By.TAG_NAME, "div")
            if phone_divs:
                phones = [p.text.strip() for p in phone_divs if p.text.strip()]
            else:
                phones = [phone_container.text.strip()]
        except: pass
        
        phone_str = ", ".join(phones) if phones else "-"

        address_profile = "-"
        try:
            address_profile = driver.find_element(By.ID, "address-value").text.strip().replace('\n', ', ')
        except:
            try:
                address_profile = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ê–¥—Ä–µ—Å')]/following-sibling::div[contains(@class, 'value')]").text.strip()
            except: pass

        work_time = "-"
        try:
            work_time = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ')]/following-sibling::div[contains(@class, 'value')]").text.strip()
        except: pass

        email = "-"
        try:
            email = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ï–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞ –ø–æ—â–∞')]/following-sibling::div[contains(@class, 'value')]").text.strip()
        except: pass

        website = "-"
        try:
            website = driver.find_element(By.XPATH, "//div[contains(@class, 'label') and contains(text(), '–ò–Ω—Ç–µ—Ä–Ω–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü–∞')]/following-sibling::div[contains(@class, 'value')]//a").get_attribute("href")
        except: pass

        basic_info.update({
            "–ê–¥—Ä–µ—Å (–ü—Ä–æ—Ñ–∏–ª)": address_profile,
            "–¢–µ–ª–µ—Ñ–æ–Ω–∏": phone_str,
            "–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ": work_time,
            "Email": email,
            "Website": website,
            "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

        return basic_info

    except Exception as e:
        print(f"üíÄ –ì—Ä–µ—à–∫–∞ –≤ –ø—Ä–æ—Ñ–∏–ª–∞ (–∞–Ω–¥–∏–±—É–ª –º–æ—Ä–∫–æ–≤ —Å–∏—Ç—É–∞—Ü–∏—è): {e}")
        return basic_info

# --- üìú MAIN LOOP (THE GRIND) ---
page = 1
# –ù—è–º–∞ max_pages, —à–µ—Ñ–µ. Until the wheels fall off.
print("üöÄ –°—Ç–∞—Ä—Ç–∏—Ä–∞–º –º–∞—à–∏–Ω–∞—Ç–∞. Fanum tax on the data.")

try:
    while True:
        target_url = f"https://www.rating.hapche.bg/search/lekari-spetsialisti/-/-&page={page}"
        print(f"\nüìÑ --- –°–¢–†–ê–ù–ò–¶–ê {page} ---")
        driver.get(target_url)
        
        try:
            # –ß–∞–∫–∞–º–µ —Ç–∞–±–ª–∏—Ü–∞—Ç–∞ –∏–ª–∏ —Å—ä–æ–±—â–µ–Ω–∏–µ, —á–µ –Ω—è–º–∞ –Ω–∏—â–æ
            try:
                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.mr-table")))
            except:
                print("‚õî –ù—è–º–∞ —Ç–∞–±–ª–∏—Ü–∞. –ú–∞–π —Å—Ç–∏–≥–Ω–∞—Ö–º–µ –∫—Ä–∞—è –∏–ª–∏ –Ω–∏ –±–∞–Ω–Ω–∞—Ö–∞ –∫–∞—Ç–æ –Ω—É–±–æ–≤–µ.")
                break

            rows = driver.find_elements(By.CSS_SELECTOR, "table.mr-table tbody tr")
            
            if not rows:
                print("‚õî –ö—Ä–∞–π –Ω–∞ –º–∞—á–∞. –ù—è–º–∞ –ø–æ–≤–µ—á–µ –¥–æ–∫—Ç–æ—Ä—á–æ–≤—Ü–∏.")
                break

            print(f"üîé –ù–∞–º–µ—Ä–∏—Ö {len(rows)} –ø—Ä–æ—Ñ–∏–ª—á–æ–≤—Ü–∏ –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞.")
            
            doctors_on_page = []
            for row in rows:
                try:
                    name_el = row.find_element(By.CSS_SELECTOR, "td.name a")
                    name = name_el.text.strip()
                    url = name_el.get_attribute("href")
                    
                    city = "-"
                    try:
                        details = row.find_element(By.CSS_SELECTOR, "td.name span").text
                        if "–≥—Ä." in details:
                            city = details.split("–≥—Ä.")[1].split(",")[0].strip()
                            city = "–≥—Ä. " + city
                    except: pass

                    doc_data = {
                        "–ò–º–µ": name,
                        "URL": url,
                        "–ì—Ä–∞–¥ (–¢–∞–±–ª–∏—Ü–∞)": city
                    }
                    doctors_on_page.append(doc_data)
                except: continue

            # –í–ª–∏–∑–∞–º–µ –≤—ä–≤ –≤—Å–µ–∫–∏ (Grindset mode activated)
            for doc in doctors_on_page:
                if "search" in doc['URL']: continue
                full_data = scrape_details_from_profile(doc['URL'], doc)
                save_single_record(full_data)

            page += 1
            
        except Exception as e:
            print(f"ü§¨ –ì–†–ï–®–ö–ê –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
            # –ê–∫–æ –≥—Ä—ä–º–Ω–µ –≥–µ–Ω–µ—Ä–∞–ª–Ω–æ, –ø–æ-–¥–æ–±—Ä–µ –¥–∞ —Å–ø—Ä–µ–º –¥–∞ –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–º
            break

finally:
    try:
        driver.quit()
        print("üõë –°–ø—Ä—è—Ö –∫–æ–ª–∞—Ç–∞.")
    except: pass
    print(f"\nüèÅ –§–∏–Ω–∏—Ç–æ! –í—Å–∏—á–∫–æ –µ –≤ –ø–∞–ø–∫–∞—Ç–∞ '{output_dir}'. Bye bye, mogger.")
